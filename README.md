# üîç E-commerce Competitive Intelligence Tool

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Platform](https://img.shields.io/badge/platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey)](https://github.com)

A production-ready Python tool for analyzing e-commerce websites (Shopify, WooCommerce, BigCommerce, etc.) and generating actionable competitive intelligence reports‚Äîwith AI-powered insights in Chinese (‰∏≠Êñá) for cross-border merchants.

---

## üåü Features

### Core Capabilities
- **Multi-Platform Detection** ‚Äî Automatically identifies Shopify, WooCommerce, Magento, BigCommerce, Wix, Squarespace, and custom platforms
- **Comprehensive Data Extraction** ‚Äî Products, prices, promotions, shipping/return policies, trust signals, and analytics tracking
- **Chinese Insights (ÂÖ≥ÈîÆÊ¥ûÂØü)** ‚Äî Generates actionable bullet points and recommendations in Chinese
- **Change Detection** ‚Äî Monitor mode tracks price changes, new products, and policy updates between scans
- **Ethical Crawling** ‚Äî Respects `robots.txt` and implements rate limiting by default

### ü§ñ AI-Powered Analysis
- **Local LLM Integration** ‚Äî Uses Qwen2.5 via Ollama (free, private, no API costs)
- **Pricing Prediction** ‚Äî ML-based optimal pricing recommendations using XGBoost
- **Threat Scoring** ‚Äî Automated competitor threat assessment (1-10 scale)
- **Brand Positioning Analysis** ‚Äî Deep analysis of competitive positioning and strategy

---

## üìã Table of Contents
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Usage](#-usage)
- [Configuration](#-configuration)
- [Output Format](#-output-format)
- [Architecture](#-architecture)
- [AI Setup](#-ai-setup-optional)
- [Extending the Tool](#-extending-the-tool)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)

---

## üíª Installation

### Prerequisites
- Python 3.10 or higher
- pip (Python package manager)
- (Optional) [Ollama](https://ollama.com/) for AI-powered analysis

### Windows (Recommended)

```powershell
# Clone the repository
git clone https://github.com/yourusername/ecommerce-intel.git
cd ecommerce-intel

# Run the setup script (creates virtual environment & installs dependencies)
.\setup.ps1

# Optional: Install Playwright for JavaScript-heavy sites
.\setup.ps1 -InstallPlaywright
```

### Linux / macOS

```bash
# Clone the repository
git clone https://github.com/yourusername/ecommerce-intel.git
cd ecommerce-intel

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Optional: Install Playwright for JS rendering
pip install playwright
playwright install chromium
```

---

## üöÄ Quick Start

### 1. Add Target Sites
Edit `sites.txt` with the e-commerce URLs you want to analyze:

```
https://allbirds.com
https://gymshark.com
https://fashionnova.com
```

### 2. Run the Tool

**Windows:**
```powershell
.\run.ps1
```

**Linux/macOS:**
```bash
python main.py --sites sites.txt
```

### 3. View Reports
Reports are generated in the `output/` folder:
- `report.md` ‚Äî Full Markdown report with insights
- `report.csv` ‚Äî Data export for spreadsheets

---

## üìñ Usage

### Command Line Options

```bash
python main.py --help

Options:
  -s, --sites TEXT       Path to sites.txt file (required)
  -o, --outdir TEXT      Output directory [default: output]
  --render-js            Use Playwright for JavaScript rendering
  --ignore-robots        Ignore robots.txt restrictions
  -t, --timeout INTEGER  Request timeout in seconds [default: 30]
  --max-products INTEGER Maximum products to extract per site [default: 50]
  --max-pages INTEGER    Maximum pages to crawl per site [default: 20]
  -m, --monitor          Enable change detection mode
  --ai                   Enable AI-powered analysis (requires Ollama)
  --ai-model TEXT        Ollama model to use [default: qwen2.5:7b]
  -v, --verbose          Enable verbose logging
```

### Example Commands

```bash
# Basic analysis
python main.py --sites sites.txt

# With AI-powered insights
python main.py --sites sites.txt --ai

# JavaScript-heavy sites (Shopify 2.0, React SPAs)
python main.py --sites sites.txt --render-js

# Monitor mode (track changes over time)
python main.py --sites sites.txt --monitor

# Full analysis with all features
python main.py --sites sites.txt --ai --render-js --monitor --verbose
```

### Windows PowerShell Shortcuts

```powershell
# Basic run
.\run.ps1

# With AI analysis
.\run.ps1 -AI

# With JavaScript rendering
.\run.ps1 -RenderJS

# All features
.\run.ps1 -AI -RenderJS -Monitor -Verbose
```

---

## ‚öôÔ∏è Configuration

Create a `config.yaml` file based on `config.example.yaml`:

```yaml
# Request settings
timeout: 30
rate_limit_delay: 1.5  # Seconds between requests

# Crawling limits
max_products: 50
max_pages: 20

# Feature flags
render_js: false
ignore_robots: false

# Output
output_dir: output
db_path: data/intel_cache.db
```

---

## üìä Output Format

### Markdown Report (`output/report.md`)

For each analyzed site, the report includes:

| Section | Description |
|---------|-------------|
| üîç **ÂÖ≥ÈîÆÊ¥ûÂØü** (Key Insights) | 6-12 AI-generated bullet points in Chinese |
| üí° **ÂèØÂ§çÂà∂ÁöÑÂä®‰ΩúÂª∫ËÆÆ** (Recommendations) | 3 actionable items for your store |
| ‚ö†Ô∏è **Á´û‰∫âÂ®ÅËÉÅËØÑ‰º∞** (Threat Score) | 1-10 rating with explanation |
| üí∞ **ÂÆö‰ª∑Á≠ñÁï•Âª∫ËÆÆ** (Pricing Advice) | Optimal price recommendations |
| üìã **Êï∞ÊçÆËØÅÊçÆ** (Evidence) | Extracted links and values |
| üìà **ÂèòÂåñËøΩË∏™** (Changes) | Price/product changes (monitor mode) |

### Comparison Table

| ÁΩëÁ´ô | Âπ≥Âè∞ | ÂìÅÁâåÂÆö‰Ωç | ‰ª∑Ê†ºÂå∫Èó¥ | ‰øÉÈîÄÊâãÊÆµ | Â®ÅËÉÅËØÑÂàÜ |
|------|------|----------|----------|----------|----------|
| allbirds.com | Shopify | ÁéØ‰øùÈûãÂ±•ÂìÅÁâå | $98-$160 | ‰ºöÂëòÊäòÊâ£„ÄÅËäÇÊó•‰øÉÈîÄ | 8/10 |

### Data Extracted Per Site

| Category | Data Points |
|----------|-------------|
| **Platform** | Shopify, WooCommerce, Magento, BigCommerce, etc. |
| **Brand** | Name, positioning, meta description |
| **Products** | Titles, URLs, prices, images, categories |
| **Pricing** | Min/max/median prices, currency, discount patterns |
| **Promotions** | Announcement bars, popups, email capture, discount codes |
| **Programs** | Loyalty, referral, subscriptions, bundles |
| **Shipping** | Free threshold, regions, delivery times |
| **Returns** | Return window, conditions, free returns |
| **Trust Signals** | Review platforms, ratings, payment badges, certifications |
| **Analytics** | Google Analytics, Meta Pixel, TikTok Pixel, Klaviyo |

---

## üèóÔ∏è Architecture

```
ecommerce-intel/
‚îú‚îÄ‚îÄ main.py                    # CLI entry point & orchestration
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py            # Module exports
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Data models (dataclasses)
‚îÇ   ‚îú‚îÄ‚îÄ http_client.py         # HTTP client with rate limiting & retries
‚îÇ   ‚îú‚îÄ‚îÄ robots_checker.py      # robots.txt compliance
‚îÇ   ‚îú‚îÄ‚îÄ url_utils.py           # URL normalization & validation
‚îÇ   ‚îú‚îÄ‚îÄ platform_detector.py   # E-commerce platform detection
‚îÇ   ‚îú‚îÄ‚îÄ page_discovery.py      # Sitemap parsing & link crawling
‚îÇ   ‚îú‚îÄ‚îÄ product_extractor.py   # Product data extraction
‚îÇ   ‚îú‚îÄ‚îÄ promo_extractor.py     # Promotion & discount detection
‚îÇ   ‚îú‚îÄ‚îÄ policy_extractor.py    # Shipping/returns policy parsing
‚îÇ   ‚îú‚îÄ‚îÄ trust_extractor.py     # Trust signals & reviews
‚îÇ   ‚îú‚îÄ‚îÄ category_extractor.py  # Product category detection
‚îÇ   ‚îú‚îÄ‚îÄ snapshot_store.py      # SQLite caching & change detection
‚îÇ   ‚îú‚îÄ‚îÄ insight_generator.py   # Chinese insight generation
‚îÇ   ‚îú‚îÄ‚îÄ ai_analyzer.py         # AI/ML analysis (Ollama + XGBoost)
‚îÇ   ‚îî‚îÄ‚îÄ report_writer.py       # Markdown & CSV report generation
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_modules.py        # Unit tests
‚îú‚îÄ‚îÄ data/                      # SQLite cache & ML models
‚îú‚îÄ‚îÄ output/                    # Generated reports
‚îú‚îÄ‚îÄ sites.txt                  # Target URLs
‚îú‚îÄ‚îÄ config.example.yaml        # Configuration template
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ setup.ps1                  # Windows setup script
‚îú‚îÄ‚îÄ run.ps1                    # Windows run script
‚îî‚îÄ‚îÄ README.md
```

---

## ü§ñ AI Setup (Optional)

AI analysis uses **Qwen2.5** running locally via **Ollama**‚Äîfree, private, and no API costs.

### Step 1: Install Ollama

Download and install from: https://ollama.com/download

### Step 2: Pull the Model

```bash
# Default model (7B parameters, ~4GB VRAM)
ollama pull qwen2.5:7b

# Larger model for better quality (14B parameters, ~8GB VRAM)
ollama pull qwen2.5:14b
```

### Step 3: Verify Installation

```bash
# List available models
ollama list

# Test the model
ollama run qwen2.5:7b "Hello, can you respond in Chinese?"
```

### Step 4: Run with AI

```bash
python main.py --sites sites.txt --ai

# Or with a specific model
python main.py --sites sites.txt --ai --ai-model qwen2.5:14b
```

---

## üîß Extending the Tool

### Adding a New Extractor

1. Create a new module in `modules/`:

```python
# modules/my_extractor.py
from bs4 import BeautifulSoup
from typing import Dict, Any

def extract_my_data(soup: BeautifulSoup, html_str: str) -> Dict[str, Any]:
    """Extract custom data from page."""
    result = {}
    
    # Your extraction logic here
    element = soup.select_one('.my-selector')
    if element:
        result['my_field'] = element.get_text(strip=True)
    
    return result
```

2. Export in `modules/__init__.py`:

```python
from .my_extractor import extract_my_data
```

3. Call from `main.py` in the `analyze_site()` function.

### Adding Platform Detection

Edit `modules/platform_detector.py`:

```python
Platform.NEW_PLATFORM: {
    'html': [r'pattern1', r'pattern2'],
    'scripts': [r'script_pattern\.js'],
    'headers': [('x-powered-by', 'NewPlatform')],
    'meta': [('generator', r'NewPlatform.*', False)],
}
```

---

## üîç Troubleshooting

### Common Issues

| Issue | Solution |
|-------|----------|
| **SSL Errors** | Tool automatically retries without SSL verification |
| **403 Forbidden** | Site blocking bots‚Äîtry `--render-js` or check `robots.txt` |
| **Timeout** | Increase with `--timeout 60` |
| **No products found** | Site uses heavy JavaScript‚Äîuse `--render-js` |
| **Ollama not found** | Ensure Ollama is running: `ollama serve` |

### Debug Mode

```bash
python main.py --sites sites.txt --verbose
```

### PowerShell Execution Policy (Windows)

If scripts are blocked:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

---

## ü§ù Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup

```bash
# Install dev dependencies
pip install -r requirements.txt
pip install pytest pytest-cov black isort

# Run tests
pytest tests/ -v

# Format code
black .
isort .
```

---

## üìÑ License

This project is licensed under the MIT License‚Äîsee the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) for HTML parsing
- [Ollama](https://ollama.com/) for local LLM inference
- [Qwen2.5](https://github.com/QwenLM/Qwen2.5) for Chinese language AI
- [Rich](https://rich.readthedocs.io/) for beautiful terminal output

---

## üì¨ Contact

- **Issues**: [GitHub Issues](https://github.com/yourusername/ecommerce-intel/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/ecommerce-intel/discussions)

---

<p align="center">
  Made with ‚ù§Ô∏è for cross-border e-commerce merchants
</p>
